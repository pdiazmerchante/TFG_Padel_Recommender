"""
Pipeline completo de limpieza y validaci√≥n de datos de p√°del.
------------------------------------------------------------
1Ô∏è‚É£ Elimina archivos intermedios antiguos (seguridad)
2Ô∏è‚É£ Lee CSV(s) desde data/raw/
3Ô∏è‚É£ Colapsa filas de un mismo evento (sin limpiar antes)
4Ô∏è‚É£ Limpia datos resultantes del colapso (min√∫sculas, tipos, normalizaci√≥n)
5Ô∏è‚É£ Valida estructura y tipos sobre df_clean
6Ô∏è‚É£ Guarda resultados intermedios y finales
"""

import sys
from pathlib import Path
import pandas as pd
import json

# === Acceso a src/ ===
ROOT = Path(__file__).resolve().parents[1]
if str(ROOT) not in sys.path:
    sys.path.append(str(ROOT))

# === Imports de tus m√≥dulos ===
from src.common.logging_setup import setup_logging
from src.data.load_data import load_raw_data, load_config
from src.data.event_collapse import collapse_events
from src.data.validate_raw import validate_raw
from src.data.clean_data import clean_dataset
from src.data.schemas import RAW_SCHEMA  # si tienes un esquema espec√≠fico para 'clean', c√°mbialo aqu√≠


def main():
    logger = setup_logging()
    logger.info("üöÄ Iniciando pipeline completo de limpieza")

    # --- 0Ô∏è‚É£ LIMPIAR INTERMEDIOS ANTIGUOS ---
    interim_dir = Path("data/interim")
    interim_dir.mkdir(parents=True, exist_ok=True)
    metadata_dir = Path("data/metadata")
    metadata_dir.mkdir(parents=True, exist_ok=True)

    for f in interim_dir.glob("*.parquet"):
        try:
            f.unlink()
        except Exception as e:
            logger.warning(f"No se pudo borrar {f}: {e}")
    for f in metadata_dir.glob("*.json"):
        try:
            f.unlink()
        except Exception as e:
            logger.warning(f"No se pudo borrar {f}: {e}")

    logger.info("üßπ Limpieza previa: eliminados archivos intermedios antiguos.")

    # --- 1Ô∏è‚É£ LEER CSV(s) ---
    _ = load_config("config/config.toml")  # mantener si tu loader necesita side-effects
    df_raw = load_raw_data("config/config.toml")
    logger.info(f"RAW: {len(df_raw):,} filas, {len(df_raw.columns)} columnas")

    # --- 2Ô∏è‚É£ COLAPSAR EVENTOS (sin limpiar antes) ---
    try:
        df_collapsed = collapse_events(df_raw)
        logger.info(f"COLLAPSED: {len(df_collapsed):,} filas, {len(df_collapsed.columns)} columnas")
    except Exception as e:
        logger.error(f"‚ùå Error al colapsar eventos: {e}")
        return

    # --- 3Ô∏è‚É£ LIMPIEZA (min√∫sculas, tipos, normalizaci√≥n) SOBRE COLLAPSED ---
    try:
        df_clean = clean_dataset(df_collapsed)
        logger.info(f"CLEAN: {len(df_clean):,} filas, {len(df_clean.columns)} columnas")
    except Exception as e:
        logger.error(f"‚ùå Error durante la limpieza: {e}")
        return

    # --- 4Ô∏è‚É£ VALIDACI√ìN (sobre df_clean) ---
    try:
        report = validate_raw(df_clean, RAW_SCHEMA)  # usa el esquema apropiado si tienes uno para 'clean'
        with open(metadata_dir / "quality_report.json", "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        logger.info("üßæ Reporte de calidad guardado en data/metadata/quality_report.json")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Validaci√≥n fall√≥ parcialmente: {e}")

    # --- 5Ô∏è‚É£ GUARDAR RESULTADOS ---
    out_raw = interim_dir / "raw_concat.parquet"             # salida directa del merge de CSVs
    out_collapsed = interim_dir / "events_collapsed.parquet" # tras collapse (sin limpiar)
    out_clean = interim_dir / "final_clean.parquet"          # tras cleaning (min√∫sculas, tipos, etc.)

    # Guardamos cada etapa claramente
    try:
        # Nota: df_raw puede ser muy grande; si prefieres no guardar, comenta esta l√≠nea
        df_raw.to_parquet(out_raw, index=False)
        df_collapsed.to_parquet(out_collapsed, index=False)
        df_clean.to_parquet(out_clean, index=False)
    except Exception as e:
        logger.error(f"‚ùå Error al guardar salidas: {e}")
        return

    logger.info("‚úÖ Guardados:")
    logger.info(f"   ‚Üí {out_raw}")
    logger.info(f"   ‚Üí {out_collapsed}")
    logger.info(f"   ‚Üí {out_clean}")
    logger.info("üéØ Pipeline completo terminado correctamente.")


if __name__ == "__main__":
    main()
